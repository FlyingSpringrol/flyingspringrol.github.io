<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding-bottom: 100px;
    padding-top: 100px;
    width: 80vw;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Old Standard TT', serif;
    color: black;
    background-color: white;
  }
  img:hover{
    opacity: .7;
  }
  h1, h2, h3, h4 {
    font-family: 'Old Standard TT', serif;
  }
</style>
<title>CS194-26 Proj3: Image Blending</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Old+Standard+TT" rel="stylesheet">
</head>
<body>
  <h1 align="middle">CS194-26 Proj 3: Image blending</h1>  
  <h2 align="middle">Brian Aronowitz: 3032201719, cs194-26-aeh</h2>  
    <div align = "middle">
      <table style="width=100%">
         <tr>
          <td>
            <img src="./pt1-3/spacegiraffes.png" align="middle" width="800px"/>
          </td>
          <td>
        </tr>
      </table>
    </div>
  <h2 align="middle">Part 1-1: Unsharp mask</h2>  
    <div align = "middle">
      <p>
        In part 1 we implement a basic unsharp mask. 
        The equation is original + weight * (image - gaussian_blurred(original)).
        Taking an image and subtracting a low pass filter leaves an image that consists of the high frequencies of the image. These high frequencies correspond to edges. By readding them back into the image, this creates an edge sharpening filter. 
      </p>
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-1/gir_blurred.png" align="middle" width="400px"/>
            <figcaption align="middle">Blurred Giraffe</figcaption>
          </td>
          <td>
            <img src="pt1-1/gir_mask_sigp05.png" align="middle" width="400px"/>
            <figcaption align="middle">Image - gaussian_blurred (sig .5)</figcaption>
          </td>
          <td>
            <img src="pt1-1/gir_unsharp.png" align="middle" width="400px"/>
            <figcaption align="middle">Result</figcaption>
          </td>
        </tr>
      </table>
  </div>

  <h2 align="middle">Part 1-2: Hybrid images</h2>  
    <div align = "middle">
      <p>
        In part 1-2, we create hybrid images described in the SIGGRAPH 2006 paper by Oliva, Torralba, and Schyns. The idea is to exploit the fact that humans cannot perceive high frequencies at certain scales and distances. If you extract the high frequencies from one image, and overlay them on the low frequencies of another, to create a hybrid image. This hybrid image will look like the high frequency image at close distances, where high frequencies dominate human perception. At far distances, and small scales, the high frequencies will fade away, and only the low-frequency image can be seen. 
        <br/>
        <br/>
        The way the two images are created is by extracting the high frequencies of one, and removing the high frequencies of the other. The low frequency image is created by performing a gaussian blur on it, the important variable here is the `sigma1`, which denotes the spread of the gaussian in the 2D kernel. 
        The high frequency image is created by subtracting the blurred image from the original image. The blurr of the image is denoted by `sigma2` in the following figures, which once again corresponds to the spread of the gaussian.
      </p>
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/meatball.jpg" align="middle" width="300px"/>
            <figcaption align="middle">Nutmeg</figcaption>
          </td>
          <td>
            <img src="pt1-2/derek.jpg" align="middle" width="300px"/>
            <figcaption align="middle">Derek</figcaption>
          </td>
        </tr>
      </table>
      <h3 align = "middle"> Sigma variations</h3>
      <p>
        By varying the sigma's, you can achieve different blends of the images. By decreasing the sigma of the high frequency image, you can also decrease the spread of the gaussian, and therefore it becomes a sharper edge filter. Below I vary the sigmas to show how the blending of the images can change.
      </p>
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/dsig1sig1.png" align="middle" width="200px"/>
            <figcaption align="middle">Sigma1 = 1, Sigma2 = 1</figcaption>
          </td>
          <td>
            <img src="pt1-2/dsig40sig5.png" align="middle" width="200px"/>
            <figcaption align="middle">Sigma1 = 40, Sigma2 = 5</figcaption>
          </td>
          <td>
            <img src="pt1-2/dsig5sig40.png" align="middle" width="200px"/>
            <figcaption align="middle">Sigma1 = 5, Sigma2 = 40</figcaption>
          </td>
        </tr>
      </table>

      <h3>
        Successful blend 1
      </h3>
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/meveryAmused.jpg" align="middle" width="400px"/>
            <figcaption align="middle">High frequency image</figcaption>
          </td>
          <td>
            <img src="pt1-2/menotAmused.jpg" align="middle" width="400px"/>
            <figcaption align="middle">Low frequency image.</figcaption>
          </td>
          <td>
            <img src="pt1-2/meSlightlyAmused.png" align="middle" width="400px"/>
            <figcaption align="middle">Final blend.</figcaption>
          </td>
        </tr>
      </table>
      <h3>
        Successful blend 2, with Fourier analysis
      </h3>
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/nyday.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-2/nynight.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-2/nydaynight.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
    </div>

    <div align = "middle">
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/nyim1.png" align="middle" width="200px"/>
            <figcaption align="middle">Fourier analysis of NY during day</figcaption>
          </td>
          <td>
            <img src="pt1-2/nyim2.png" align="middle" width="200px"/>
            <figcaption align="middle">Fourier analysis of NY during night</figcaption>
          </td>
          <td>
            <img src="pt1-2/nyp1.png" align="middle" width="200px"/>
            <figcaption align="middle">Fourier analysis of blurred gaussian.</figcaption>
          </td>
          <td>
            <img src="pt1-2/nyp2.png" align="middle" width="200px"/>
            <figcaption align="middle">Fourier analysis of high freq photo.</figcaption>
          </td>
          <td>
            <img src="pt1-2/nyfinal.png" align="middle" width="200px"/>
            <figcaption align="middle">Fourier analysis of final blend.</figcaption>
          </td>
        </tr>
      </table>

      <h3> Failure blend. </h3>

        <p>
         After spending probably two hours trying to get Tommy Wiseau's face onto a potato I called it a failure. It turns out if you make the high frequency filter too agressive, it also picks up quite a bit of low frequencies, dominating the photo at all scales and distances.
        </p> 
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/Wiseau.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-2/potato.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-2/tmsig1.png" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
  </div>

  <h2 align = "middle"> Part 1-3: Gaussian and Laplacian stacks</h2>
    <p>For part 1-3, we implemented gaussian and laplacian stacks. These stacks are created by recursively applying the processes outlined in part 1-1 and 1-2. The gaussian stacks result in images that are the original, with more and more high frequencies filtered out. Said in plain english, the gaussian stack is a stack of more and more blurry images.
      <br/>
     The laplacian stacks are similar. The top of the stack represents the highest frequencies, while the lower ones represent lower frequencies. </p>
    <div align = "middle">
      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/gaussny.png" align="middle" height="200px"/>
            <figcaption align="middle">From part 1-2, the gaussian stack of new york.</figcaption>
          </td>
        </tr>
         <tr>
          <td>
            <img src="pt1-2/laplaceny.png" align="middle" height="150px"/>
            <figcaption align="middle">From part 1-2, the laplacian stack of new york.</figcaption>
          </td>
        </tr>
      </table>

      <table style="width=100%">
         <tr>
          <td>
            <img src="pt1-2/gaussDali.png" align="middle" height="300px"/>
            <figcaption align="middle">The gaussian stack of Dali's painting.</figcaption>
          </td>
        </tr>
         <tr>
          <td>
            <img src="pt1-2/laplaceDali.png" align="middle" height="200px"/>
            <figcaption align="middle">The laplacian stack of Dali's painting.</figcaption>
          </td>
        </tr>
      </table>
  </div>
  <h2 align = "middle"> Part 1-4: Multiresolution blending</h2>
  <div align = "middle">
    <p>
      In part 1-4 I implemented the multiresolution blending algorithm as described in the 1983 paper by Burt and Adelson. Essentially the idea is to blend images across their laplacian and gaussian stacks, using a 'weight' that corresponds to a blurred mask at each step in the stack. 
    </p>
      </h3> The algorithm takes 3 inputs: The first image, the second image, and the mask.</h3>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/headEyeImage.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/me.png" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/headEye.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
     <p> Below I show the laplacian stack levels of the images. There also are corresponding gaussian stacks for both the first and second images, as well as gaussian stacks for the mask, and it's inverse. It's a total of six data structures for everything. </p>
      <table align = "middle" style="width=100%">
        <tr>
          <td>
            <img src="pt1-3/headlapleft.png" align="middle" width="1000px"/>
            <figcaption align="middle">Laplacian stack for eye.</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="pt1-3/headlapright.png" align="middle" width="1000px"/>
            <figcaption align="middle">Laplacian stack for me.</figcaption>
          </td>
        </tr>
      </table>
      <p>The algorithm works by multiplying the corresponding gaussian stacks and laplacian stacks together from each image, and averaging them at each level. You end up with a nice blend around the edges of the image. Here is the final result. </p>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/eyeInTheForehead.png" align="middle" width="500px"/>
          </td>
        </tr>
      </table>
      <h3>Below is a another set of inputs to the algorithm. </h3>
      <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/galaxy.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/gmask.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/g_source.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
      <h3> Final result: Titled, 'maybe were not alone after all...'</h3>
      <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/spacegiraffes.png" align="middle" width="500px"/>
          </td>
        </tr>
      </table>
      <h3> Here are some attempts at replicating the or-apple, messing around with different masks and sigmas.</h3>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/orapple1.png" align="middle" width="200px"/>
          </td>
          <td>
            <img src="pt1-3/orapple2.png" align="middle" width="200px"/>
          </td>
          <td>
            <img src="pt1-3/orapple3.png" align="middle" width="200px"/>
          </td>
        </tr>
      </table>
  </div>
    <h2 align="middle">Part 2-1: The Toy Problem</h2>  
    <div align = "middle">
      <p> 
        For part 2 we implemented poisson blending. This algorithm works by matching the gradients in the target image to the gradients in the intended source.
        In part 2-1 we implemented a simple case, where we want to reconstruct an image completely. For this, we only need right to left direction gradients, and up to down direction gradients. We solve using least squares. 
      </p>
      <h3> Results </h3>
      <p> My reconstructed toy image, was darker than the original. This may have been because of several things. I reduced the atol and btol of my scipy.sparse.lsqr solver, which possibly may have effected reconstruction. Secondly, it's possible when displaying the image that my imshow messed up the colors. In theory, you have enough information to perfectly reconstruct the pixels, so I'm assuming it was one of those two things. </p>
       <table align = "middle" style="width=100%">
          <tr>
            <td>
              <img src="pt2/actual.jpg" align="middle" width="200px"/>
            </td>
            <td>
              <img src="pt2/toy.jpg" align="middle" width="200px"/>
            </td>
          </tr>
        </table>

    </div>
    <div align = "middle">
      <h2 align="middle">Part 2-2: Poisson blending</h2>  
        <p> 
          For part 2-2 we implement real poisson blending. To accomplish this, you need three inputs, a target, a source, and a mask. 
        </p>
         <table align = "middle" style="width=100%">
            <tr>
              <td>
                <img src="pt2/arm_source.jpg" align="middle" width="200px"/>
              </td>
              <td>
                <img src="pt2/armMask.jpg" align="middle" width="200px"/>
              </td>
              <td>
                <img src="pt2/armTarget.jpg" align="middle" width="200px"/>
              </td>
            </tr>
        </table>
        <p> 
          The idea is to match the pixels in the mask regions to the gradients of the source. I set up my A matrix by filling it with -1s and 1s, to match to my B vector, which is populated with my intended source gradient values. The boundary conditions on the mask (where pixel gradients 'fall off' the edge) involves weighting the intended final values with the target image pixel values. We once again solve for 'x' with least squares. This creates smooth transitions along the edges, with the error distributed among the center pixels.  
          <br/>
          <br/>
          To optimize my algorithm, I needed to crop out the bounding box of the region I wanted to blend. I then solve for this region using scipy.sparse.ltsq. Afterwards, I blend this region back into the image.
        </p>
        <h3>Result of previous inputs</h3>
          <table align = "middle" style="width=100%">
            <tr>
              <td>
                <img src="pt2/armcopy.jpg" align="middle" width="400px"/>
                <figcaption align="middle">Direct pixel copying</figcaption>

              </td>   
              <td>
                <img src="pt2/armTat.jpg" align="middle" width="400px"/>
                <figcaption align="middle">Blended</figcaption>

              </td>
          
            </tr>
        </table>
        <h3>Comparison between Laplacian and Poisson Blending</h3>
        <p> Inputs: </p>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt1-3/headEyeImage.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/me.png" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt1-3/headEye.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
      <h3> Results </h3>
       <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt2/headFinal.jpg" align="middle" width="400px"/>
            <figcaption align="middle">Poisson Blend</figcaption>

          </td>
          <td>
            <img src="pt1-3/eyeInTheForehead.png" align="middle" width="400px"/>
            <figcaption align="middle">Laplacian Blend</figcaption>
          </td>
        </tr>
      </table>
          <p>
            The difference between these was subtle, based on the fact that it was transferring two images from very similar backgrounds. That being said, the advantage of using Poisson over Laplacian blending is pretty evident. Poisson blending allows for more seamless color matching of the source onto the target image. This is also unfortunately it's undoing as well, as transferring color between objects with vastly different colors causes radical color changes in the source image. 
          </p>
         <h3> A nice result </h3>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt2/starmask.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt2/starsource.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt2/startarget.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt2/starsfinal.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
         <h3> Failure case </h3>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt2/sunmask.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt2/suntarget.jpg" align="middle" width="400px"/>
          </td>
          <td>
            <img src="pt2/sunsrc.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>
         <p>
          As highlighted earlier, it's large changes in background colors that can break the poisson algorithm. There is simply too much 'error' to distribute among the pixels and it can be clearly seen in the results. 
        </p>
        <table align = "middle" style="width=100%">
         <tr>
          <td>
            <img src="pt2/sunfinal.jpg" align="middle" width="400px"/>
          </td>
        </tr>
      </table>

      <h2> 
        Conclusion
      </h2>
      <p>I felt like I was fighting python over the entire course of the project, from the scipy functions, to sparse matrices, to lack of debugging functionality. I really milked it for optimization, but in the end my image sizes were limited to about 300 by 300 based on the least squares implementation. This project did get me thinking about pixels as an abstraction of visual information. To be honest, I still haven't seen any compelling generative art or compositing projects that rely purely on the pixel abstraction. I'm inclined to believe it's because the pixel, and images, in their naive form, have lost most of their expressive power by the time they reach the [255,255,255] representation. </p>
    </div>
</html>
