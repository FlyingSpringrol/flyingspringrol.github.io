<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding-bottom: 100px;
    padding-top: 100px;
    width: 80vw;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Old Standard TT', serif;
    color: black;
    background-color: white;
  }
  h1, h2, h3, h4 {
    font-family: 'Old Standard TT', serif;
  }
</style>
<title>CS194-26 Proj5: Lightfields</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Old+Standard+TT" rel="stylesheet">
</head>
<body>
  <h1 align="middle">CS194-26 Proj5: Lightfields</h1>  
  <h2 align="middle">Brian Aronowitz: 3032201719, cs194-26-aeh</h2>  
  <h2 align="middle">Part 1: Depth Refocusing</h2>  

    <div align = "middle">
      <p>
        In part 1 we implement depth refocusing. This can be done by taking all the camera positions in the gantry, and essentially simulating what would happen if there was a single camera focused on an object. You can do this simulation by taking the camera positions, and shifting their images then averaging the total images to obtain a simulated DOF image. To compute shifts, you simply calculate the average position of the cameras, (which all have offsets relative to a reference point), then shifting their images based on how far way they are from the reference. More generally, this is t * (cam_center-cam_av), where t is a variable controlling the shifts. Below are some results.
      </p>
      <table style="width=100%">
         <tr>
          <td>
            <img src="amethyst.gif" align="middle" width="400px"/>
            <figcaption align="middle">Amethyst: t varying from -.5 to .5</figcaption>
          </td>
        </tr>
      </table>
      <table style="width=100%">
         <tr>
          <td>
            <img src="legosShift.gif" align="middle" width="400px"/>
            <figcaption align="middle">Legos: t varying from -.5 to .5</figcaption>
          </td>
        </tr>
      </table>
  </div>
  <h2 align="middle">Part 2: Aperture Adjustment</h2>  
  <div align = "middle">
      <p>
        In part 2 we implement aperture adjustment. This simply involves only including pixels from cameras that are inside the radius around the center of the grid. This allows for simulation of DOF. The lower the radius, and thus the less camera pixels you are averaging, the less the effect of DOF, as there is less variation of pixels and therefore less blur. Below are some results.
      </p>
      <table style="width=100%">
         <tr>
          <td>
            <img src="legosAP.gif" align="middle" width="400px"/>
            <figcaption align="middle">Lego: varying radius from 0-20 grid units</figcaption>
          </td>
        </tr>
      </table>
      <table style="width=100%">
         <tr>
          <td>
            <img src="tarotAP.gif" align="middle" width="400px"/>
            <figcaption align="middle">Tarot: varying radius from 0-20 grid units</figcaption>
          </td>
        </tr>
      </table>
  </div>
  <h2 align="middle">Summary</h2>  
  <div align = "middle">
      <p>
        This was an interesting foray into lightfields, I'm thinking about exploring them more for my final project, as my favorite feature of them (the panning left and right) wasn't implemented in this project. Seeing refractions and reflections change in a lightfield when you move around is quite crazy to me. There's a lot of cool research being done into lightfield videos and compression, as well as VR display of lightfields, and I'm excited to see how it pans out in the next decade or so.
      </p>
  </div>
</html>
