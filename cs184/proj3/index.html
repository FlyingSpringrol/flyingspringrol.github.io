<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Brian Aronowitz  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<style>  
div.padded {  
  padding-top: 0px;  
  padding-right: 100px;  
  padding-bottom: 0.25in;  
  padding-left: 100px;  
}  
body {
padding: 100px;
width: 1000px;
margin: auto;
text-align: left;
font-weight: 300;
font-family: 'Open Sans', sans-serif;
color: #121212;
}
h1, h2, h3, h4 {
font-family: 'Source Sans Pro', sans-serif;
}
</style> 
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<br />
<h1 align="middle">Assignment 3-2: PathTracer</h1>
    <h2 align="middle">Brian Aronowitz: CS184-agr</h2>
    <div align = "middle">
        <table style="width=100%">
          <tr>
            <td>
              <img src="images/pt1/cbspheres-100.png" align="middle" width="400px"/>
              <figcaption align="middle"></figcaption>
            </td>
            <td>
              <img src="images/pt4/dragon_depth3.png" align="middle" width="400px"/>
              <figcaption align="middle"></figcaption>
            </td>
              
          </tr>
      </table>
    </div>
    <div class="padded">
        <h2 align="middle">Overview</h2>

        <p> In part 2 of our raytracer, we began implementing more complex materials,  </p>

    <h2 align="middle">Part 1: Reflective and Refractive Materials</h2>
        <p>In part 1 I implemented reflective and refractive materials. Reflective materials simply involved reflecting an incoming ray across the intsection normal point: and due to the fact that all the w_in and w_out vectors are already being converted to the local coordinate space of the intersection, this is as easy as flipping the reflected vector to be -in.x, -in.y, in.z, and you have your out vector. The pdf for these calcuations is always 1 because there is a 100% chance of this occuring. (This is an implementation of a perfect mirror.) Refractive materials were definitely a little bit trickier. </p>

        <p> To implement refractive materials, I used Snell's equations in spherical coordinates to calculate potential refractive rays of the input rays. I say 'potential' because every ray that hits the sphere doesn't necessarily refract every time, it can actually do a couple things: it can reflect off the surface if it's at a certain angle, it can refract, or it can totally internally reflect. To simulate this behavior, Iuse a couple tricks in a couple steps. The first step is to calculate if a ray refracts or reflects using Snell's equations. If this ray did indeed refract, I then use something called Schlicks approximation, which allows us to get the probability of a ray doing a 'total internal reflection', which essentially means it bounces around inside the sphere instead of refracting out of it. Schlicks approximation returns an 'R' value, which I factor into our returned PDF and reflectance values, which gives us close to physically accurate light behaviour. </p>

        <div align="center">
            <h3 align = "center">Spheres with rendered with increasing ray-depths, prob of termination set at .1 <br/> Spp = 1024 </h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt1/cbspheres-0.png" align="middle" width="400px"/>
                    <figcaption align="middle">0 bounces (only direct lighting from light sources seen) <br/> </figcaption>
                  </td>
                  <td>
                    <img src="images/pt1/cbspheres-1.png" align="middle" width="400px"/>
                    <figcaption align="middle">1 bounce (single bounce, no contribution from ref/refract)</figcaption>
                  </td>
                    
                </tr>
                <tr>
                  <td>
                    <img src="images/pt1/cbspheres-2.png" align="middle" width="400px"/>
                    <figcaption align="middle">2 bounces, reflection/refraction begins to contribute, no internal reflection</figcaption>
                  </td>
                  <td>
                    <img src="images/pt1/cbspheres-3.png" align="middle" width="400px"/>
                    <figcaption align="middle">3 bounces, total internal reflection begins to play a part</figcaption>
                  </td>
                    
                </tr>
                <tr>
                  <td>
                    <img src="images/pt1/cbspheres-4.png" align="middle" width="400px"/>
                    <figcaption align="middle">4 bounces, appearance of noisey transmission artifact on base of sphere. </figcaption>
                  </td>
                  <td>
                    <img src="images/pt1/cbspheres-5.png" align="middle" width="400px"/>
                    <figcaption align="middle">Noisey transmission gets less noisey, sphere begins to refract on the wall as well. </figcaption>
                  </td>
                    
                </tr>
            </table>
        </div>
        <div align="center">
            <table style="width=100%">
                <tr>
                  <td align = "center">
                    <img src="images/pt1/cbspheres-100.png" align="middle" width="400px"/>
                    <figcaption align="middle">100 bounces: very similar to 5 bounces, but the light transmissions are less noisey, and the reflected light in the glass sphere has a nice halo effect.</figcaption>
                  </td>
                </tr>
            </table>
        </div>
    <h2 align="middle">Part 2: Microfacet Material</h2>
        <p> For this section I implemented microfacet materials. The key to microfacet materials is this equation right here: which contains a fresnel term, a shadow-masking term, a normal distribution term, and some scaling factors. </p>
        <div align="center">
            <table style="width=100%">
                <tr>
                  <td align = "center">
                    <img src="images/f_function.png" align="middle" width="400px"/>
                    <figcaption align="middle"></figcaption>
                  </td>
                </tr>
            </table>
        </div>
        <p> The key aspects of this part were simply figuring out how to sample the normals, (what makes microfacet materials and metals appear the way they do is the multi-directional distribution of the normals), and implementing the Fresnel and Normal distribution functions correctly. 
        <br/></br>
        The first section of the part was implementing Cosine sampling, which actually was suprisingly easy due to the simplicity of the normal sampling. For cosine sampling, you can simply get your normal by sampling on a uniform hemisphere, calculate your half-vector, and plug and chug through the Fresnel, Shadow, and Normal terms. 

        <br/><br/>

        The second section of this part was quite a bit trickier: it involved importance sampling for the normals. Essentially, this is done by matching the randomly sampled normals with the normal distribution function term (D(h)). This matching is done by putting randomly sampled x and y points from [0,1] through a separate function which biases their values towards more frequently sampled normal vectors on the normal distribution curve. This allows for sampling of more common normals. After calculating the normal using this process in each call to sample_f I proceed in much the same way as with cosine sampling: I reflect the sampled wi vector around the calculated normal then plug into the F, D, and G functions and return the calculated reflectance. 
        </p>
        <div align="center">
            <h3 align = "center">Dragon rendered with different alpha values, rendered at 1024 spp</h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt2/dragona-005.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .005, highly glossy, high noise</figcaption>
                  </td>
                  <td>
                    <img src="images/pt2/dragona-05.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .05, less gloss, less noise</figcaption>
                  </td>
                    
                </tr>
                <tr>
                  <td>
                    <img src="images/pt2/dragona-25.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .25, less gloss, even less noise</figcaption>
                  </td>
                  <td>
                    <img src="images/pt2/dragona-5.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .5, least gloss, least noise.</figcaption>
                  </td>
                    
                </tr>
            </table>
        </div>
        <div align="center">
            <h3>Dragon rendered with different materials</h3>
            <p>Mercury: r: n = 1.8795 , k = 5.1076,<br/>
                        g: n = 1.5472,  k = 4.6437,<br/>
                        b: n = 1.1353 , k = 3.9980 <br/>
                        alpha = .03
            </p>
            <p> 
                Silver:   r: n = 0.059193, k = 4.1283 <br/>
                  g: n = 0.059881, k = 3.5892 <br/>
                  b: n = 0.047366, k = 2.8132 <br/>
                  alpha = .3
            </p>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt2/dragon_mercury.png" align="middle" width="400px"/>
                    <figcaption align="middle">Mercury</figcaption>
                  </td>
                  <td>
                    <img src="images/pt2/dragon_silver.png" align="middle" width="400px"/>
                    <figcaption align="middle">Silver: </figcaption>
                  </td>
                </tr>
            </table>
        </div>
        <div align="center">
            <h3>Cosine vs importance sampling comparison</h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt2/bunny-cosine.png" align="middle" width="400px"/>
                    <figcaption align="middle">Cosine</figcaption>
                  </td>
                  <td>
                    <img src="images/pt2/bunny-importance.png" align="middle" width="400px"/>
                    <figcaption align="middle">Importance</figcaption>
                  </td>
                </tr>
            </table>
            <p></p>
        </div>

<h2 align="middle">Part 3: Environment Sampling</h2>
    <p> Part 3 involved implementing environment lights, which effectively serve as a false sky of sorts. It works similarly to normal area-light sampling: a call to get light::sample() and returns a Spectrum and a wi direction. Where the environment light functions differently is in the sampling details. Instead of uniformly sampling a white light, (like a standard area light) environment lights sample a texture. <br/> <br/>
      For the first section on this part, uniform cosine sampling, the sampling was reasonably easy to implement. Simply generate a random vector on a sphere, convert it to spherical coordinates, then convert it to flattened xy coordinates. Once you have this vector, you can then index into the texture and return the spectrum value at this coordinate. In effect, you just randomly sample on a texture and call it a day. 
      <br/> <br/>
      The next section, importance sampling, was quite the opposite of cosine sampling and was quite hard to implement. The concept behind importance sampling is to sample the brightest parts of the image, the idea being that these bright areas would contribute the most to the illumination of the object. To do this, I first construct two primary data arrays. The first array is the dimension of the height of the image, and it holds the marginal distribution of each row of the image. The second array is the size of the whole image,  and holds the conditional distribution for each pixel in the image. I use the values stored in these arrays to calculate the x and y's that I will be using index into the texture. Once the two data arrays are created, on each call to light::sample, I start by creating a random [0,1] sample for x and y. Next, to select the row I want to use, I iterate down the cond_y array, and the first probability value I hit that is greater than our sampled y, I select that row. Next, I iterate through the row, and if any value in the row is greater than our sampled x, I have our x value. I then use this x and y to index into the texture and sample a spectrum. See, definitely harder than cosine sampling.

     </p>
    <div align= "center">
        <h3>Uffizi exr</h3>
        <table style="width=100%">
            <tr>
              <td align = "center">
                <img src="images/pt3/uffizi.png" align="middle" width="800px"/>
                <figcaption align="middle">Uffizi.exr</figcaption>
              </td>
            </tr>
        </table>
    </div>
    <div align= "center">
      <h3> Uffizi probability debug </h3>
        <table style="width=100%">
            <tr>
              <td align = "center">
                <img src="images/pt3/probability_debug.png" align="middle" width="800px"/>
                <figcaption align="middle">Probability debug for uffizi</figcaption>
              </td>
            </tr>
        </table>
    </div>

    <div align="center">
      <h3> Unlit bunny cosine vs importance comparison. 4 spp, 64 spl</h3>
          <table style="width=100%">
            <tr>
              <td>
                <img src="images/pt3/bunnyCosine.png" align="middle" width="400px"/>
                <figcaption align="middle">Bunny cosine sampling</figcaption>
              </td>
              <td>
                <img src="images/pt3/bunnyImportance.png" align="middle" width="400px"/>
                <figcaption align="middle">Bunny importance sampling</figcaption>
              </td>
                
            </tr>
        </table>
          <p> For the diffuse colored bunny, importance sampling was noticeably better in all aspects. There was simply less noise everywhere.</p>
        <h3> Copper bunny cosine vs importance comparison. 4 spp, 64 spl</h3>
        <table style="width=100%">
            <tr>
              <td>
                <img src="images/pt3/bunnyCuCosine.png" align="middle" width="400px"/>
                <figcaption align="middle">Bunny cu cosine sampling</figcaption>
              </td>
              <td>
                <img src="images/pt3/bunnyCuImportance.png" align="middle" width="400px"/>
                <figcaption align="middle">Bunny importance sampling</figcaption>
              </td>
                
            </tr>
        </table>
        <p> 
          For the copper bunny, importance sampling actually seemed to add more noise than random hemispheric sampling. I am guessing this is because of several factors: the most important factor actually being the high brightness value of the EXR I took. I'm guessing those white dots would not disapear if I upped my sampling rate to 512 or 1024, because they are accurate representations of what the bunny would look like under the EXR. Furthermore, I think the dark spots on the cosine bunny are actually caused by the texture being sampled at low illuminance places, and therefore look dark, but will become lighter if I gave the algorithm has time to converge. In short, once again importance sampling has the edge  photorealistic environment lighting is being attempted, getting closer to ground truth in shorter amounts of time.  
        </p>
    </div>
    <h2 align="middle">Part 4: DOF</h2>
    <p> Part 4 involved simulating thin lenses to create depth of field effects. 
      <br/> </br> To simulate a thin lens, you have to do several things. Starting from the virtual sensor: you have to iterate over every single pixel on the sensor, (like I had been doing in previous parts). How to sample the ray to cast out into the scene to use for radiance calculations gets a little bit tricker though. In the past, I could just send it out arbitrarily into space, but for this part I had to simulate it traveling through a thin lens first. 
      <br/>To do this, I first simulate what a ray would do if it was being focused from the sensor through a perfect pinhole camera, and with this ray I calculate the hit point with the focal plane (determined by focal distance). Once this focal point is found, I then calculate what an actual ray would do if it was originating from the point on the sensor, and refracting through some random point on the lens. This new ray, which goes from the point on the lens to the focal intersection point, is the ray where I get my radiance information for from the pixel. Calculating the radiance with these simulated refracted rays creates a depth of field effect because if there is no object at the focus point for the specific sensor pixel, the rays being refracted through points on the lens will accumulate the average of a bunch of objects from the scene, resulting in a pleasant DOF effect</p>
    </div>
        <div align="center">
            <h3 align = "center"> Varying focal length from 1.0 to 2.3, aperture held at 0.08.</h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt4/dragon_depth1.png" align="middle" width="400px"/>
                    <figcaption align="middle">focal distance = 1.0</figcaption>
                  </td>
                  <td>
                    <img src="images/pt4/dragon_depth2.png" align="middle" width="400px"/>
                    <figcaption align="middle">focal distance = 1.4</figcaption>
                  </td>
                    
                </tr>
                <tr>
                  <td>
                    <img src="images/pt4/dragon_depth3.png" align="middle" width="400px"/>
                    <figcaption align="middle">focal distance = 1.7</figcaption>
                  </td>
                  <td>
                    <img src="images/pt4/dragon_depth4.png" align="middle" width="400px"/>
                    <figcaption align="middle">focal distance = 2.3</figcaption>
                  </td>
                    
                </tr>
            </table>
        </div>
        <div align="center">
            <h3 align = "center">Varying aperture sizes from 0 to 2.0, with constant focal length.</h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt4/dragon_ap1.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .001, d = 1.8</figcaption>
                  </td>
                  <td>
                    <img src="images/pt4/dragon_ap2.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = .05, d = 1.8</figcaption>
                  </td>
                    
                </tr>
                <tr>
                  <td>
                    <img src="images/pt4/dragon_ap3.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = 1.0, d = 1.8</figcaption>
                  </td>
                  <td>
                    <img src="images/pt4/dragon_ap4.png" align="middle" width="400px"/>
                    <figcaption align="middle">a = 2.0, d = 1.8</figcaption>
                  </td>
                    
                </tr>
            </table>
        </div>
    <h2 align="middle">Part 5: Shaders</h2>
        <p> Part 5 was realtime shading using web GLSL shaders. For the previous parts of the project, everything was being done the CPU, which resulted in lonnnnng render times. Here, framerates are 60fps or higher, but look accordingly worse. </p>
        <p> I used fragment and vertex shaders to calculate the lighting for the objects. In essence, vertex shaders take information like vertex position, color, etc, that is created at the vertex-processing stage of the graphics pipeline. The vertex shader than modifies this information if needed, for example displacing or transforming vertices into world space, then passes this transformed information to the fragment shaders. Fragment shaders essentially color each pixel on an object. Occasionally they'll color more than one pixel (which is a fragment, hence the name), but in effect they do just color an object pixel by pixel, based on the information passed by the vertex shader and shader attributes. </p>
        <p>In section 1 of part 5, I implemented a blinn-phong shading model using vertex and fragment shaders. The vertex shader essentially just transforms it's vertex normals, fragment position and gl_position to world space, while the fragment shader does all the hard work. To create the full blinn-phong effect, you need three componnets: specular, diffuse, and ambient. The ambient term is the easiest, it is essentially a low-intensity ambient light that you add to your final result to simulate global lighting. The next component to be calculated is the diffuse component, which uses lamberts half-angle law to figure out the amount of light reflected back towards the camera. The specular component is calculated by amplifying the result of the diffuse calculation to only include a small strip of light on the object, adding a specular highlight. The total blinn-phong effect emerges when you add all of them together. </p>
        <div align="center">
          <h3 align = "center"> Broken down blinn-phong shader</h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt5/ambient.png" align="middle" width="400px"/>
                    <figcaption align="middle">ambient term</figcaption>
                  </td>
                  <td>
                    <img src="images/pt5/diffuse.png" align="middle" width="400px"/>
                    <figcaption align="middle">diffuse term</figcaption>
                  </td>     
                </tr>
                <tr>
                  <td>
                    <img src="images/pt5/specular.png" align="middle" width="400px"/>
                    <figcaption align="middle">specular term</figcaption>
                  </td>
                  <td>
                    <img src="images/pt5/blinnphongt.png" align="middle" width="400px"/>
                    <figcaption align="middle">all together</figcaption>
                  </td>     
                </tr>
            </table>
        </div>
        <div align="center">
            <h3 align = "center"> Custom texture mapped object </h3>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt5/texturemapped.png" align="middle" width="800px"/>
                    <figcaption align="middle">Texture mapped with texture from part 4</figcaption>
                  </td>    
                </tr>
            </table>
        </div>
        <div align="center">
          <h3 align = "center"> Custom displacement objects </h3>
             <p> Below are my bump mapped and displacement implementations, each at 256x256 divisions. 
                Bump mapping simply simulates displacement through clever use of normal mapping, so it makes sense that it can't have that extreme of an effect on the mesh. Displacement mapping on the other hand, actually offsets the vertices, and you can see that with the jagged edges and physically moved vertices on the object.

             </p>

            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt5/bumpmapped.png" align="middle" width="400px"/>
                    <figcaption align="middle">Bump mapping</figcaption>
                  </td>
                  <td>
                    <img src="images/pt5/displacementMapped.png" align="middle" width="400px"/>
                    <figcaption align="middle">Displacement mapping</figcaption>
                  </td>     
                </tr>
            </table>

      </div> 
        <div align="center">
          <h3 align = "center"> Custom displacement objects, higher subdiv</h3>
             <p> Below are my bump mapped and displacement implementations, each at 1000x1000 divisions. </p>

             <p> The bump mapped object shows very little change when the vertex count is doubled. This intuitively makes sense, as again, it's the fragment shader that is handling the normal displacements and shading calculations on a per pixel basis for bump-mapping. The displacement shader is a different story, with higher vertex counts it shows much higher detail as far as lighting and surface quality goes. This makes sense in the context of the displacement shader, the more vertices you have to displace, the more vertices you actually displace and can see the effect of the shader on. </p>
              <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt5/bumphp.png" align="middle" width="400px"/>
                    <figcaption align="middle">Bump mapping</figcaption>
                  </td>
                  <td>
                    <img src="images/pt5/displacehp.png" align="middle" width="400px"/>
                    <figcaption align="middle">Displacement mapping</figcaption>
                  </td>     
                </tr>
            </table>
        </div>

        <div align="center">
            <h3>Custom shader </h3>
            <p> For my custom shader, I made a music visualizer, by hooking up a web audio node to some shader parameters. I take the average of certain frequency bands to drive either displacement or color. The 0-440hz range is hooked up to displacement, while the higher frequencies are hooked up to color changes. </p>
            <table style="width=100%">
                <tr>
                  <td>
                    <img src="images/pt5/music.png" align="middle" width="400px"/>
                    <figcaption align="middle">Sphere normals</figcaption>
                  </td>    
                </tr>
            </table>
        </div> 
        <div align = "center">      
        <a href="gl/index.html">Link to my part 5 code</a>
      </div>
</div>
</body>
</html>




